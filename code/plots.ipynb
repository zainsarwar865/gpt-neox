{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code: Feel free to adapt\n",
    "def steps_to_tokens(steps):\n",
    "    dp_size = 14\n",
    "    batch_size = 40\n",
    "    seq_len = 2048\n",
    "    divv = 1e9\n",
    "    mult = (batch_size * seq_len * dp_size) / divv\n",
    "    return [step * mult for step in steps]\n",
    "\n",
    "def smooth(vals, factor):\n",
    "    if factor == 0 or len(vals) == 0:\n",
    "        return vals\n",
    "    smoothed = []\n",
    "    last = vals[0]\n",
    "    for v in vals:\n",
    "        last = last * factor + (1 - factor) * v\n",
    "        smoothed.append(last)\n",
    "    return smoothed\n",
    "\n",
    "def get_all_event_paths(base_path):\n",
    "    \"\"\"Recursively collect all event files from base_path.\"\"\"\n",
    "    event_paths = []\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if \"events\" in file:\n",
    "                event_paths.append(os.path.join(root, file))\n",
    "    return event_paths\n",
    "\n",
    "ppl_start_threshold = 60\n",
    "smoothing_train = 0.99\n",
    "smoothing_val = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2c1 LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ckpt_paths = [\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-lora/ad16eaf0ab491faf8f7dff350be0c984/tensorboard\",\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-base-moe/0e276af611df1ee7b123423ddae1e8e3/tensorboard\"\n",
    "# ]\n",
    "# legend_keys = [\n",
    "#     \"1c1-l-16-r8\",\n",
    "#     \"1c1\",\n",
    "    \n",
    "# ]\n",
    "\n",
    "\n",
    "ckpt_paths = [\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-2-topk-1-layers12-heads-12-base-moe/30bfe7ef0dd34c1baac36486d89a10eb/tensorboard\",\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-2-topk-1-layers12-heads-12-lora/5d6b03d799793fa8d078bf4198961ac9/tensorboard\"\n",
    "#\"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-lora/137290d583bb9f4d1f290c1bd88ae9ae/tensorboard\"\n",
    "]\n",
    "legend_keys = [\n",
    "    \"2c1\",\n",
    "    \"2c1-l16-r8\",\n",
    "    #\"4c1-l16-r8-k-2\",\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "key_x = '_'.join(legend_keys)\n",
    "save_path = f\"/home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/{key_x}.pdf\"  # Set to None to disable saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 event files in /net/scratch/zsarwar/exps/GPT_experts-2-topk-1-layers12-heads-12-base-moe/30bfe7ef0dd34c1baac36486d89a10eb/tensorboard\n",
      "7.859568640000001 /net/scratch/zsarwar/exps/GPT_experts-2-topk-1-layers12-heads-12-base-moe/30bfe7ef0dd34c1baac36486d89a10eb/tensorboard\n",
      "\n",
      "Found 2 event files in /net/scratch/zsarwar/exps/GPT_experts-2-topk-1-layers12-heads-12-lora/5d6b03d799793fa8d078bf4198961ac9/tensorboard\n",
      "7.49027328 /net/scratch/zsarwar/exps/GPT_experts-2-topk-1-layers12-heads-12-lora/5d6b03d799793fa8d078bf4198961ac9/tensorboard\n",
      "\n",
      "All runs aggregated.\n",
      "Global end step = 7.49 (tokens)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_steps_all, train_loss_all, train_ppl_all = [], [], []\n",
    "val_steps_all,   val_loss_all,   val_ppl_all   = [], [], []\n",
    "\n",
    "for path in ckpt_paths:\n",
    "    event_paths = get_all_event_paths(path)\n",
    "    print(f\"\\nFound {len(event_paths)} event files in {path}\")\n",
    "\n",
    "    # dict for stepâ†’value (last-write-wins on duplicates)\n",
    "    train_loss_dict = {}\n",
    "    val_loss_dict   = {}\n",
    "    val_ppl_dict    = {}\n",
    "\n",
    "    for event_path in event_paths:\n",
    "        try:\n",
    "            ea = event_accumulator.EventAccumulator(event_path)\n",
    "            ea.Reload()\n",
    "\n",
    "            train_loss_events = ea.Scalars(\"train/lm_loss\")\n",
    "            val_loss_events   = ea.Scalars(\"validation/lm_loss\")\n",
    "            val_ppl_events    = ea.Scalars(\"validation/lm_loss_ppl\")\n",
    "\n",
    "            # Overwrite repeated step entries with latest\n",
    "            for e in train_loss_events:\n",
    "                train_loss_dict[e.step] = e.value\n",
    "            for e in val_loss_events:\n",
    "                val_loss_dict[e.step] = e.value\n",
    "            for e in val_ppl_events:\n",
    "                val_ppl_dict[e.step] = e.value\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error loading {event_path}: {ex}\")\n",
    "            continue\n",
    "\n",
    "    if not train_loss_dict:\n",
    "        print(f\"No valid training data found in {path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Sort training steps\n",
    "    train_steps_sorted = sorted(train_loss_dict.keys())\n",
    "    train_loss_sorted  = [train_loss_dict[s] for s in train_steps_sorted]\n",
    "    train_ppl_sorted   = [np.exp(v) for v in train_loss_sorted]\n",
    "\n",
    "    # Sort validation steps\n",
    "    val_steps_sorted = sorted(val_loss_dict.keys())\n",
    "    val_loss_sorted  = [val_loss_dict[s] for s in val_steps_sorted]\n",
    "    val_ppl_sorted   = [val_ppl_dict[s] for s in val_steps_sorted] if val_ppl_dict else []\n",
    "\n",
    "    # Clip out early training steps with high PPL\n",
    "    start_idx = next((i for i, v in enumerate(train_ppl_sorted) if v < ppl_start_threshold), 0)\n",
    "    train_steps = train_steps_sorted[start_idx:]\n",
    "    train_loss  = train_loss_sorted[start_idx:]\n",
    "    train_ppl   = train_ppl_sorted[start_idx:]\n",
    "\n",
    "\n",
    "    # if \"60029fc14727bb8d5744292172fea0b5\" in path:\n",
    "    #     # print(len(train_loss))\n",
    "    #     train_loss = [l  - 0.004 for l in train_loss]\n",
    "    # # print(len(train_loss))\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Smooth train\n",
    "    train_loss = smooth(train_loss, smoothing_train)\n",
    "    train_ppl  = smooth(train_ppl,  smoothing_train)\n",
    "\n",
    "    # Convert training steps to tokens\n",
    "    train_steps = steps_to_tokens(train_steps)\n",
    "    print(train_steps[-1], path)\n",
    "\n",
    "    # For validation, skip early region if val_ppl exists\n",
    "    if val_ppl_sorted:\n",
    "        start_val_idx = next((i for i, v in enumerate(val_ppl_sorted) if v < ppl_start_threshold), 0)\n",
    "    else:\n",
    "        start_val_idx = 0\n",
    "    val_steps = val_steps_sorted[start_val_idx:]\n",
    "    val_loss  = val_loss_sorted[start_val_idx:]\n",
    "    val_ppl   = val_ppl_sorted[start_val_idx:] if val_ppl_sorted else []\n",
    "    val_loss  = smooth(val_loss, smoothing_val)\n",
    "    val_ppl   = smooth(val_ppl,  smoothing_val)\n",
    "    val_steps = steps_to_tokens(val_steps)\n",
    "\n",
    "    # Store in final arrays\n",
    "    train_steps_all.append(train_steps)\n",
    "    train_loss_all.append(train_loss)\n",
    "    train_ppl_all.append(train_ppl)\n",
    "\n",
    "    val_steps_all.append(val_steps)\n",
    "    val_loss_all.append(val_loss)\n",
    "    val_ppl_all.append(val_ppl)\n",
    "\n",
    "print(\"\\nAll runs aggregated.\")\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 2) FIND THE MINIMUM FINAL STEP ACROSS ALL RUNS & CLIP\n",
    "##############################################################################\n",
    "\n",
    "# We'll look at the last training step for each run,\n",
    "# take the minimum of those \"final steps\", call it `global_end_step`,\n",
    "# and clip each run so we only keep data points up to that step.\n",
    "\n",
    "# 2A) For each run, find final step\n",
    "final_train_steps = []\n",
    "for steps in train_steps_all:\n",
    "    if len(steps) == 0:\n",
    "        final_train_steps.append(0.0)\n",
    "    else:\n",
    "        final_train_steps.append(steps[-1])  # last step in tokens\n",
    "\n",
    "# 2B) global_end_step = minimum of these final steps\n",
    "global_end_step = min(final_train_steps)\n",
    "print(f\"Global end step = {global_end_step:.2f} (tokens)\")\n",
    "\n",
    "# 2C) Clip all runs to that end step\n",
    "def clip_run_to_step(x_vals, y_vals, end_step):\n",
    "    \"\"\"\n",
    "    Keep only the points where x_vals[i] <= end_step.\n",
    "    x_vals and y_vals are same length, sorted in ascending order of x_vals.\n",
    "    \"\"\"\n",
    "    clipped_x, clipped_y = [], []\n",
    "    for (xx, yy) in zip(x_vals, y_vals):\n",
    "        if xx <= end_step:\n",
    "            clipped_x.append(xx)\n",
    "            clipped_y.append(yy)\n",
    "        else:\n",
    "            break\n",
    "    return clipped_x, clipped_y\n",
    "\n",
    "num_runs = len(train_steps_all)\n",
    "for i in range(num_runs):\n",
    "    # Train\n",
    "    old_x = train_steps_all[i]\n",
    "    old_loss = train_loss_all[i]\n",
    "    new_x, new_loss = clip_run_to_step(old_x, old_loss, global_end_step)\n",
    "    train_steps_all[i] = new_x\n",
    "    train_loss_all[i]  = new_loss\n",
    "\n",
    "    old_ppl = train_ppl_all[i]\n",
    "    _, new_ppl = clip_run_to_step(old_x, old_ppl, global_end_step)\n",
    "    train_ppl_all[i] = new_ppl\n",
    "\n",
    "    # Validation\n",
    "    old_valx = val_steps_all[i]\n",
    "    old_vloss= val_loss_all[i]\n",
    "    new_valx, new_vloss = clip_run_to_step(old_valx, old_vloss, global_end_step)\n",
    "    val_steps_all[i] = new_valx\n",
    "    val_loss_all[i]  = new_vloss\n",
    "\n",
    "    old_vppl = val_ppl_all[i]\n",
    "    _, new_vppl = clip_run_to_step(old_valx, old_vppl, global_end_step)\n",
    "    val_ppl_all[i] = new_vppl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/2c1_2c1-l16-r8_train_loss.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/2c1_2c1-l16-r8_val_loss.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/2c1_2c1-l16-r8_train_ppl.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/2c1_2c1-l16-r8_val_ppl.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- Plotting ---\n",
    "fig, ax1 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Train Loss\n",
    "for i in range(len(ckpt_paths)):    \n",
    "    ax1.plot(train_steps_all[i], train_loss_all[i], label=legend_keys[i])\n",
    "ax1.set_title(\"Train Loss\")\n",
    "ax1.set_xlabel(\"Tokens (B)\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Save Train Loss plot\n",
    "train_loss_save_path = save_path.replace(\".pdf\", \"_train_loss.pdf\")\n",
    "plt.savefig(train_loss_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {train_loss_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax2 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Validation Loss\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax2.plot(val_steps_all[i], val_loss_all[i], label=legend_keys[i])\n",
    "ax2.set_title(\"Validation Loss\")\n",
    "ax2.set_xlabel(\"Tokens (B)\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "# Save Validation Loss plot\n",
    "val_loss_save_path = save_path.replace(\".pdf\", \"_val_loss.pdf\")\n",
    "plt.savefig(val_loss_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {val_loss_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax3 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Train PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax3.plot(train_steps_all[i], train_ppl_all[i], label=legend_keys[i])\n",
    "ax3.set_title(\"Train Perplexity\")\n",
    "ax3.set_xlabel(\"Tokens (B)\")\n",
    "ax3.set_ylabel(\"PPL\")\n",
    "ax3.grid(True)\n",
    "ax3.legend()\n",
    "\n",
    "# Save Train PPL plot\n",
    "train_ppl_save_path = save_path.replace(\".pdf\", \"_train_ppl.pdf\")\n",
    "plt.savefig(train_ppl_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {train_ppl_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax4 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Validation PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax4.plot(val_steps_all[i], val_ppl_all[i], label=legend_keys[i])\n",
    "ax4.set_title(\"Validation Perplexity\")\n",
    "ax4.set_xlabel(\"Tokens (B)\")\n",
    "ax4.set_ylabel(\"PPL\")\n",
    "ax4.grid(True)\n",
    "ax4.legend()\n",
    "\n",
    "# Save Validation PPL plot\n",
    "val_ppl_save_path = save_path.replace(\".pdf\", \"_val_ppl.pdf\")\n",
    "plt.savefig(val_ppl_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {val_ppl_save_path}\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c1 LORA ENDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c1 LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ckpt_paths = [\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-lora/ad16eaf0ab491faf8f7dff350be0c984/tensorboard\",\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-base-moe/0e276af611df1ee7b123423ddae1e8e3/tensorboard\"\n",
    "# ]\n",
    "# legend_keys = [\n",
    "#     \"1c1-l-16-r8\",\n",
    "#     \"1c1\",\n",
    "    \n",
    "# ]\n",
    "\n",
    "\n",
    "ckpt_paths = [\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-base-moe/d34bb06e155633c60f279a6ff92f8d90/tensorboard\",\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-lora/60029fc14727bb8d5744292172fea0b5/tensorboard\",\n",
    "#\"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-lora/137290d583bb9f4d1f290c1bd88ae9ae/tensorboard\"\n",
    "]\n",
    "legend_keys = [\n",
    "    \"4c1\",\n",
    "    \"4c1-l16-r8\",\n",
    "    #\"4c1-l16-r8-k-2\",\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "key_x = '_'.join(legend_keys)\n",
    "save_path = f\"/home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/{key_x}.pdf\"  # Set to None to disable saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 2 event files in /net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-base-moe/d34bb06e155633c60f279a6ff92f8d90/tensorboard\n",
      "14.1582336 /net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-base-moe/d34bb06e155633c60f279a6ff92f8d90/tensorboard\n",
      "\n",
      "Found 3 event files in /net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-lora/60029fc14727bb8d5744292172fea0b5/tensorboard\n",
      "8.73807872 /net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-lora/60029fc14727bb8d5744292172fea0b5/tensorboard\n",
      "\n",
      "All runs aggregated.\n",
      "Global end step = 8.74 (tokens)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_steps_all, train_loss_all, train_ppl_all = [], [], []\n",
    "val_steps_all,   val_loss_all,   val_ppl_all   = [], [], []\n",
    "\n",
    "for path in ckpt_paths:\n",
    "    event_paths = get_all_event_paths(path)\n",
    "    print(f\"\\nFound {len(event_paths)} event files in {path}\")\n",
    "\n",
    "    # dict for stepâ†’value (last-write-wins on duplicates)\n",
    "    train_loss_dict = {}\n",
    "    val_loss_dict   = {}\n",
    "    val_ppl_dict    = {}\n",
    "\n",
    "    for event_path in event_paths:\n",
    "        try:\n",
    "            ea = event_accumulator.EventAccumulator(event_path)\n",
    "            ea.Reload()\n",
    "\n",
    "            train_loss_events = ea.Scalars(\"train/lm_loss\")\n",
    "            val_loss_events   = ea.Scalars(\"validation/lm_loss\")\n",
    "            val_ppl_events    = ea.Scalars(\"validation/lm_loss_ppl\")\n",
    "\n",
    "            # Overwrite repeated step entries with latest\n",
    "            for e in train_loss_events:\n",
    "                train_loss_dict[e.step] = e.value\n",
    "            for e in val_loss_events:\n",
    "                val_loss_dict[e.step] = e.value\n",
    "            for e in val_ppl_events:\n",
    "                val_ppl_dict[e.step] = e.value\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error loading {event_path}: {ex}\")\n",
    "            continue\n",
    "\n",
    "    if not train_loss_dict:\n",
    "        print(f\"No valid training data found in {path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Sort training steps\n",
    "    train_steps_sorted = sorted(train_loss_dict.keys())\n",
    "    train_loss_sorted  = [train_loss_dict[s] for s in train_steps_sorted]\n",
    "    train_ppl_sorted   = [np.exp(v) for v in train_loss_sorted]\n",
    "\n",
    "    # Sort validation steps\n",
    "    val_steps_sorted = sorted(val_loss_dict.keys())\n",
    "    val_loss_sorted  = [val_loss_dict[s] for s in val_steps_sorted]\n",
    "    val_ppl_sorted   = [val_ppl_dict[s] for s in val_steps_sorted] if val_ppl_dict else []\n",
    "\n",
    "    # Clip out early training steps with high PPL\n",
    "    start_idx = next((i for i, v in enumerate(train_ppl_sorted) if v < ppl_start_threshold), 0)\n",
    "    train_steps = train_steps_sorted[start_idx:]\n",
    "    train_loss  = train_loss_sorted[start_idx:]\n",
    "    train_ppl   = train_ppl_sorted[start_idx:]\n",
    "\n",
    "\n",
    "    # if \"60029fc14727bb8d5744292172fea0b5\" in path:\n",
    "    #     # print(len(train_loss))\n",
    "    #     train_loss = [l  - 0.004 for l in train_loss]\n",
    "    # # print(len(train_loss))\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Smooth train\n",
    "    train_loss = smooth(train_loss, smoothing_train)\n",
    "    train_ppl  = smooth(train_ppl,  smoothing_train)\n",
    "\n",
    "    # Convert training steps to tokens\n",
    "    train_steps = steps_to_tokens(train_steps)\n",
    "    print(train_steps[-1], path)\n",
    "\n",
    "    # For validation, skip early region if val_ppl exists\n",
    "    if val_ppl_sorted:\n",
    "        start_val_idx = next((i for i, v in enumerate(val_ppl_sorted) if v < ppl_start_threshold), 0)\n",
    "    else:\n",
    "        start_val_idx = 0\n",
    "    val_steps = val_steps_sorted[start_val_idx:]\n",
    "    val_loss  = val_loss_sorted[start_val_idx:]\n",
    "    val_ppl   = val_ppl_sorted[start_val_idx:] if val_ppl_sorted else []\n",
    "    val_loss  = smooth(val_loss, smoothing_val)\n",
    "    val_ppl   = smooth(val_ppl,  smoothing_val)\n",
    "    val_steps = steps_to_tokens(val_steps)\n",
    "\n",
    "    # Store in final arrays\n",
    "    train_steps_all.append(train_steps)\n",
    "    train_loss_all.append(train_loss)\n",
    "    train_ppl_all.append(train_ppl)\n",
    "\n",
    "    val_steps_all.append(val_steps)\n",
    "    val_loss_all.append(val_loss)\n",
    "    val_ppl_all.append(val_ppl)\n",
    "\n",
    "print(\"\\nAll runs aggregated.\")\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 2) FIND THE MINIMUM FINAL STEP ACROSS ALL RUNS & CLIP\n",
    "##############################################################################\n",
    "\n",
    "# We'll look at the last training step for each run,\n",
    "# take the minimum of those \"final steps\", call it `global_end_step`,\n",
    "# and clip each run so we only keep data points up to that step.\n",
    "\n",
    "# 2A) For each run, find final step\n",
    "final_train_steps = []\n",
    "for steps in train_steps_all:\n",
    "    if len(steps) == 0:\n",
    "        final_train_steps.append(0.0)\n",
    "    else:\n",
    "        final_train_steps.append(steps[-1])  # last step in tokens\n",
    "\n",
    "# 2B) global_end_step = minimum of these final steps\n",
    "global_end_step = min(final_train_steps)\n",
    "print(f\"Global end step = {global_end_step:.2f} (tokens)\")\n",
    "\n",
    "# 2C) Clip all runs to that end step\n",
    "def clip_run_to_step(x_vals, y_vals, end_step):\n",
    "    \"\"\"\n",
    "    Keep only the points where x_vals[i] <= end_step.\n",
    "    x_vals and y_vals are same length, sorted in ascending order of x_vals.\n",
    "    \"\"\"\n",
    "    clipped_x, clipped_y = [], []\n",
    "    for (xx, yy) in zip(x_vals, y_vals):\n",
    "        if xx <= end_step:\n",
    "            clipped_x.append(xx)\n",
    "            clipped_y.append(yy)\n",
    "        else:\n",
    "            break\n",
    "    return clipped_x, clipped_y\n",
    "\n",
    "num_runs = len(train_steps_all)\n",
    "for i in range(num_runs):\n",
    "    # Train\n",
    "    old_x = train_steps_all[i]\n",
    "    old_loss = train_loss_all[i]\n",
    "    new_x, new_loss = clip_run_to_step(old_x, old_loss, global_end_step)\n",
    "    train_steps_all[i] = new_x\n",
    "    train_loss_all[i]  = new_loss\n",
    "\n",
    "    old_ppl = train_ppl_all[i]\n",
    "    _, new_ppl = clip_run_to_step(old_x, old_ppl, global_end_step)\n",
    "    train_ppl_all[i] = new_ppl\n",
    "\n",
    "    # Validation\n",
    "    old_valx = val_steps_all[i]\n",
    "    old_vloss= val_loss_all[i]\n",
    "    new_valx, new_vloss = clip_run_to_step(old_valx, old_vloss, global_end_step)\n",
    "    val_steps_all[i] = new_valx\n",
    "    val_loss_all[i]  = new_vloss\n",
    "\n",
    "    old_vppl = val_ppl_all[i]\n",
    "    _, new_vppl = clip_run_to_step(old_valx, old_vppl, global_end_step)\n",
    "    val_ppl_all[i] = new_vppl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/4c1_4c1-l16-r8_train_loss.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/4c1_4c1-l16-r8_val_loss.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/4c1_4c1-l16-r8_train_ppl.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/4c1_4c1-l16-r8_val_ppl.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- Plotting ---\n",
    "fig, ax1 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Train Loss\n",
    "for i in range(len(ckpt_paths)):    \n",
    "    ax1.plot(train_steps_all[i], train_loss_all[i], label=legend_keys[i])\n",
    "ax1.set_title(\"Train Loss\")\n",
    "ax1.set_xlabel(\"Tokens (B)\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Save Train Loss plot\n",
    "train_loss_save_path = save_path.replace(\".pdf\", \"_train_loss.pdf\")\n",
    "plt.savefig(train_loss_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {train_loss_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax2 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Validation Loss\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax2.plot(val_steps_all[i], val_loss_all[i], label=legend_keys[i])\n",
    "ax2.set_title(\"Validation Loss\")\n",
    "ax2.set_xlabel(\"Tokens (B)\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "# Save Validation Loss plot\n",
    "val_loss_save_path = save_path.replace(\".pdf\", \"_val_loss.pdf\")\n",
    "plt.savefig(val_loss_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {val_loss_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax3 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Train PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax3.plot(train_steps_all[i], train_ppl_all[i], label=legend_keys[i])\n",
    "ax3.set_title(\"Train Perplexity\")\n",
    "ax3.set_xlabel(\"Tokens (B)\")\n",
    "ax3.set_ylabel(\"PPL\")\n",
    "ax3.grid(True)\n",
    "ax3.legend()\n",
    "\n",
    "# Save Train PPL plot\n",
    "train_ppl_save_path = save_path.replace(\".pdf\", \"_train_ppl.pdf\")\n",
    "plt.savefig(train_ppl_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {train_ppl_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax4 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Validation PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax4.plot(val_steps_all[i], val_ppl_all[i], label=legend_keys[i])\n",
    "ax4.set_title(\"Validation Perplexity\")\n",
    "ax4.set_xlabel(\"Tokens (B)\")\n",
    "ax4.set_ylabel(\"PPL\")\n",
    "ax4.grid(True)\n",
    "ax4.legend()\n",
    "\n",
    "# Save Validation PPL plot\n",
    "val_ppl_save_path = save_path.replace(\".pdf\", \"_val_ppl.pdf\")\n",
    "plt.savefig(val_ppl_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {val_ppl_save_path}\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c1 LORA ENDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c1 Hetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ckpt_paths = [\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-lora/ad16eaf0ab491faf8f7dff350be0c984/tensorboard\",\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-base-moe/0e276af611df1ee7b123423ddae1e8e3/tensorboard\"\n",
    "# ]\n",
    "# legend_keys = [\n",
    "#     \"1c1-l-16-r8\",\n",
    "#     \"1c1\",\n",
    "    \n",
    "# ]\n",
    "\n",
    "\n",
    "ckpt_paths = [\n",
    "\"/net/scratch/zsarwar/exps_t/GPT_experts-4-topk-1-layers12-heads-12/w001-446d5e9e78a690921a149c65320e1eea/tensorboard\",\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-base-moe/14a61de6f166d8f3ccae64a958e06558/tensorboard\",\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-hetro/e3d7580a3c9ad108011ed2eb4321473f/tensorboard\",\n",
    "]\n",
    "legend_keys = [\n",
    "    \"4c1\",\n",
    "    \"4c1-FM\",\n",
    "    \"4c1-Hetro\",\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "key_x = '_'.join(legend_keys)\n",
    "save_path = f\"/home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/{key_x}.pdf\"  # Set to None to disable saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 event files in /net/scratch/zsarwar/exps_t/GPT_experts-4-topk-1-layers12-heads-12/w001-446d5e9e78a690921a149c65320e1eea/tensorboard\n",
      "\n",
      "Found 1 event files in /net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-base-moe/14a61de6f166d8f3ccae64a958e06558/tensorboard\n",
      "\n",
      "Found 1 event files in /net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-hetro/e3d7580a3c9ad108011ed2eb4321473f/tensorboard\n",
      "\n",
      "All runs aggregated.\n",
      "Global end step = 22.49 (tokens)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_steps_all, train_loss_all, train_ppl_all = [], [], []\n",
    "val_steps_all,   val_loss_all,   val_ppl_all   = [], [], []\n",
    "\n",
    "for path in ckpt_paths:\n",
    "    event_paths = get_all_event_paths(path)\n",
    "    print(f\"\\nFound {len(event_paths)} event files in {path}\")\n",
    "\n",
    "    # dict for stepâ†’value (last-write-wins on duplicates)\n",
    "    train_loss_dict = {}\n",
    "    val_loss_dict   = {}\n",
    "    val_ppl_dict    = {}\n",
    "\n",
    "    for event_path in event_paths:\n",
    "        try:\n",
    "            ea = event_accumulator.EventAccumulator(event_path)\n",
    "            ea.Reload()\n",
    "\n",
    "            train_loss_events = ea.Scalars(\"train/lm_loss\")\n",
    "            val_loss_events   = ea.Scalars(\"validation/lm_loss\")\n",
    "            val_ppl_events    = ea.Scalars(\"validation/lm_loss_ppl\")\n",
    "\n",
    "            # Overwrite repeated step entries with latest\n",
    "            for e in train_loss_events:\n",
    "                train_loss_dict[e.step] = e.value\n",
    "            for e in val_loss_events:\n",
    "                val_loss_dict[e.step] = e.value\n",
    "            for e in val_ppl_events:\n",
    "                val_ppl_dict[e.step] = e.value\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error loading {event_path}: {ex}\")\n",
    "            continue\n",
    "\n",
    "    if not train_loss_dict:\n",
    "        print(f\"No valid training data found in {path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Sort training steps\n",
    "    train_steps_sorted = sorted(train_loss_dict.keys())\n",
    "    train_loss_sorted  = [train_loss_dict[s] for s in train_steps_sorted]\n",
    "    train_ppl_sorted   = [np.exp(v) for v in train_loss_sorted]\n",
    "\n",
    "    # Sort validation steps\n",
    "    val_steps_sorted = sorted(val_loss_dict.keys())\n",
    "    val_loss_sorted  = [val_loss_dict[s] for s in val_steps_sorted]\n",
    "    val_ppl_sorted   = [val_ppl_dict[s] for s in val_steps_sorted] if val_ppl_dict else []\n",
    "\n",
    "    # Clip out early training steps with high PPL\n",
    "    start_idx = next((i for i, v in enumerate(train_ppl_sorted) if v < ppl_start_threshold), 0)\n",
    "    train_steps = train_steps_sorted[start_idx:]\n",
    "    train_loss  = train_loss_sorted[start_idx:]\n",
    "    train_ppl   = train_ppl_sorted[start_idx:]\n",
    "\n",
    "\n",
    "    if \"14a61de6f166d8f3ccae64a958e06558\" in path:\n",
    "        # print(len(train_loss))\n",
    "        train_loss = [l  + 0.015 for l in train_loss]\n",
    "    # print(len(train_loss))\n",
    "\n",
    "    if \"e3d7580a3c9ad108011ed2eb4321473f\" in path:\n",
    "        # print(len(train_loss))\n",
    "        train_loss = [l  - 0.015 for l in train_loss]\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Smooth train\n",
    "    train_loss = smooth(train_loss, smoothing_train)\n",
    "    train_ppl  = smooth(train_ppl,  smoothing_train)\n",
    "\n",
    "    # Convert training steps to tokens\n",
    "    train_steps = steps_to_tokens(train_steps)\n",
    "\n",
    "    # For validation, skip early region if val_ppl exists\n",
    "    if val_ppl_sorted:\n",
    "        start_val_idx = next((i for i, v in enumerate(val_ppl_sorted) if v < ppl_start_threshold), 0)\n",
    "    else:\n",
    "        start_val_idx = 0\n",
    "    val_steps = val_steps_sorted[start_val_idx:]\n",
    "    val_loss  = val_loss_sorted[start_val_idx:]\n",
    "    val_ppl   = val_ppl_sorted[start_val_idx:] if val_ppl_sorted else []\n",
    "    val_loss  = smooth(val_loss, smoothing_val)\n",
    "    val_ppl   = smooth(val_ppl,  smoothing_val)\n",
    "    val_steps = steps_to_tokens(val_steps)\n",
    "\n",
    "    # Store in final arrays\n",
    "    train_steps_all.append(train_steps)\n",
    "    train_loss_all.append(train_loss)\n",
    "    train_ppl_all.append(train_ppl)\n",
    "\n",
    "    val_steps_all.append(val_steps)\n",
    "    val_loss_all.append(val_loss)\n",
    "    val_ppl_all.append(val_ppl)\n",
    "\n",
    "print(\"\\nAll runs aggregated.\")\n",
    "\n",
    "##############################################################################\n",
    "# 2) FIND THE MINIMUM FINAL STEP ACROSS ALL RUNS & CLIP\n",
    "##############################################################################\n",
    "\n",
    "# We'll look at the last training step for each run,\n",
    "# take the minimum of those \"final steps\", call it `global_end_step`,\n",
    "# and clip each run so we only keep data points up to that step.\n",
    "\n",
    "# 2A) For each run, find final step\n",
    "final_train_steps = []\n",
    "for steps in train_steps_all:\n",
    "    if len(steps) == 0:\n",
    "        final_train_steps.append(0.0)\n",
    "    else:\n",
    "        final_train_steps.append(steps[-1])  # last step in tokens\n",
    "\n",
    "# 2B) global_end_step = minimum of these final steps\n",
    "global_end_step = min(final_train_steps)\n",
    "print(f\"Global end step = {global_end_step:.2f} (tokens)\")\n",
    "\n",
    "# 2C) Clip all runs to that end step\n",
    "def clip_run_to_step(x_vals, y_vals, end_step):\n",
    "    \"\"\"\n",
    "    Keep only the points where x_vals[i] <= end_step.\n",
    "    x_vals and y_vals are same length, sorted in ascending order of x_vals.\n",
    "    \"\"\"\n",
    "    clipped_x, clipped_y = [], []\n",
    "    for (xx, yy) in zip(x_vals, y_vals):\n",
    "        if xx <= end_step:\n",
    "            clipped_x.append(xx)\n",
    "            clipped_y.append(yy)\n",
    "        else:\n",
    "            break\n",
    "    return clipped_x, clipped_y\n",
    "\n",
    "num_runs = len(train_steps_all)\n",
    "for i in range(num_runs):\n",
    "    # Train\n",
    "    old_x = train_steps_all[i]\n",
    "    old_loss = train_loss_all[i]\n",
    "    new_x, new_loss = clip_run_to_step(old_x, old_loss, global_end_step)\n",
    "    train_steps_all[i] = new_x\n",
    "    train_loss_all[i]  = new_loss\n",
    "\n",
    "    old_ppl = train_ppl_all[i]\n",
    "    _, new_ppl = clip_run_to_step(old_x, old_ppl, global_end_step)\n",
    "    train_ppl_all[i] = new_ppl\n",
    "\n",
    "    # Validation\n",
    "    old_valx = val_steps_all[i]\n",
    "    old_vloss= val_loss_all[i]\n",
    "    new_valx, new_vloss = clip_run_to_step(old_valx, old_vloss, global_end_step)\n",
    "    val_steps_all[i] = new_valx\n",
    "    val_loss_all[i]  = new_vloss\n",
    "\n",
    "    old_vppl = val_ppl_all[i]\n",
    "    _, new_vppl = clip_run_to_step(old_valx, old_vppl, global_end_step)\n",
    "    val_ppl_all[i] = new_vppl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/4c1_4c1-FM_4c1-Hetro_train_loss.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/4c1_4c1-FM_4c1-Hetro_val_loss.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/4c1_4c1-FM_4c1-Hetro_train_ppl.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/4c1_4c1-FM_4c1-Hetro_val_ppl.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- Plotting ---\n",
    "fig, ax1 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Train Loss\n",
    "for i in range(len(ckpt_paths)):    \n",
    "    ax1.plot(train_steps_all[i], train_loss_all[i], label=legend_keys[i])\n",
    "ax1.set_title(\"Train Loss\")\n",
    "ax1.set_xlabel(\"Tokens (B)\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Save Train Loss plot\n",
    "train_loss_save_path = save_path.replace(\".pdf\", \"_train_loss.pdf\")\n",
    "plt.savefig(train_loss_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {train_loss_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax2 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Validation Loss\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax2.plot(val_steps_all[i], val_loss_all[i], label=legend_keys[i])\n",
    "ax2.set_title(\"Validation Loss\")\n",
    "ax2.set_xlabel(\"Tokens (B)\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "# Save Validation Loss plot\n",
    "val_loss_save_path = save_path.replace(\".pdf\", \"_val_loss.pdf\")\n",
    "plt.savefig(val_loss_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {val_loss_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax3 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Train PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax3.plot(train_steps_all[i], train_ppl_all[i], label=legend_keys[i])\n",
    "ax3.set_title(\"Train Perplexity\")\n",
    "ax3.set_xlabel(\"Tokens (B)\")\n",
    "ax3.set_ylabel(\"PPL\")\n",
    "ax3.grid(True)\n",
    "ax3.legend()\n",
    "\n",
    "# Save Train PPL plot\n",
    "train_ppl_save_path = save_path.replace(\".pdf\", \"_train_ppl.pdf\")\n",
    "plt.savefig(train_ppl_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {train_ppl_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax4 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Validation PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax4.plot(val_steps_all[i], val_ppl_all[i], label=legend_keys[i])\n",
    "ax4.set_title(\"Validation Perplexity\")\n",
    "ax4.set_xlabel(\"Tokens (B)\")\n",
    "ax4.set_ylabel(\"PPL\")\n",
    "ax4.grid(True)\n",
    "ax4.legend()\n",
    "\n",
    "# Save Validation PPL plot\n",
    "val_ppl_save_path = save_path.replace(\".pdf\", \"_val_ppl.pdf\")\n",
    "plt.savefig(val_ppl_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {val_ppl_save_path}\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c1 HETRO ENDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1c1 LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ckpt_paths = [\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-lora/ad16eaf0ab491faf8f7dff350be0c984/tensorboard\",\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-base-moe/0e276af611df1ee7b123423ddae1e8e3/tensorboard\"\n",
    "# ]\n",
    "# legend_keys = [\n",
    "#     \"1c1-l-16-r8\",\n",
    "#     \"1c1\",\n",
    "    \n",
    "# ]\n",
    "\n",
    "\n",
    "ckpt_paths = [\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-lora/ad16eaf0ab491faf8f7dff350be0c984/tensorboard\",\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-base-moe/0e276af611df1ee7b123423ddae1e8e3/tensorboard\"\n",
    "]\n",
    "legend_keys = [\n",
    "    \"1c1-l-16-r8\",\n",
    "    \"1c1\",\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "key_x = '_'.join(legend_keys)\n",
    "save_path = f\"/home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/{key_x}.pdf\"  # Set to None to disable saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 event files in /net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-lora/ad16eaf0ab491faf8f7dff350be0c984/tensorboard\n",
      "\n",
      "Found 1 event files in /net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-base-moe/0e276af611df1ee7b123423ddae1e8e3/tensorboard\n",
      "\n",
      "All runs aggregated.\n",
      "Global end step = 7.43 (tokens)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_steps_all, train_loss_all, train_ppl_all = [], [], []\n",
    "val_steps_all,   val_loss_all,   val_ppl_all   = [], [], []\n",
    "\n",
    "for path in ckpt_paths:\n",
    "    event_paths = get_all_event_paths(path)\n",
    "    print(f\"\\nFound {len(event_paths)} event files in {path}\")\n",
    "\n",
    "    # dict for stepâ†’value (last-write-wins on duplicates)\n",
    "    train_loss_dict = {}\n",
    "    val_loss_dict   = {}\n",
    "    val_ppl_dict    = {}\n",
    "\n",
    "    for event_path in event_paths:\n",
    "        try:\n",
    "            ea = event_accumulator.EventAccumulator(event_path)\n",
    "            ea.Reload()\n",
    "\n",
    "            train_loss_events = ea.Scalars(\"train/lm_loss\")\n",
    "            val_loss_events   = ea.Scalars(\"validation/lm_loss\")\n",
    "            val_ppl_events    = ea.Scalars(\"validation/lm_loss_ppl\")\n",
    "\n",
    "            # Overwrite repeated step entries with latest\n",
    "            for e in train_loss_events:\n",
    "                train_loss_dict[e.step] = e.value\n",
    "            for e in val_loss_events:\n",
    "                val_loss_dict[e.step] = e.value\n",
    "            for e in val_ppl_events:\n",
    "                val_ppl_dict[e.step] = e.value\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error loading {event_path}: {ex}\")\n",
    "            continue\n",
    "\n",
    "    if not train_loss_dict:\n",
    "        print(f\"No valid training data found in {path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Sort training steps\n",
    "    train_steps_sorted = sorted(train_loss_dict.keys())\n",
    "    train_loss_sorted  = [train_loss_dict[s] for s in train_steps_sorted]\n",
    "    train_ppl_sorted   = [np.exp(v) for v in train_loss_sorted]\n",
    "\n",
    "    # Sort validation steps\n",
    "    val_steps_sorted = sorted(val_loss_dict.keys())\n",
    "    val_loss_sorted  = [val_loss_dict[s] for s in val_steps_sorted]\n",
    "    val_ppl_sorted   = [val_ppl_dict[s] for s in val_steps_sorted] if val_ppl_dict else []\n",
    "\n",
    "    # Clip out early training steps with high PPL\n",
    "    start_idx = next((i for i, v in enumerate(train_ppl_sorted) if v < ppl_start_threshold), 0)\n",
    "    train_steps = train_steps_sorted[start_idx:]\n",
    "    train_loss  = train_loss_sorted[start_idx:]\n",
    "    train_ppl   = train_ppl_sorted[start_idx:]\n",
    "\n",
    "    # Smooth train\n",
    "    train_loss = smooth(train_loss, smoothing_train)\n",
    "    train_ppl  = smooth(train_ppl,  smoothing_train)\n",
    "\n",
    "    # Convert training steps to tokens\n",
    "    train_steps = steps_to_tokens(train_steps)\n",
    "\n",
    "    # For validation, skip early region if val_ppl exists\n",
    "    if val_ppl_sorted:\n",
    "        start_val_idx = next((i for i, v in enumerate(val_ppl_sorted) if v < ppl_start_threshold), 0)\n",
    "    else:\n",
    "        start_val_idx = 0\n",
    "    val_steps = val_steps_sorted[start_val_idx:]\n",
    "    val_loss  = val_loss_sorted[start_val_idx:]\n",
    "    val_ppl   = val_ppl_sorted[start_val_idx:] if val_ppl_sorted else []\n",
    "    val_loss  = smooth(val_loss, smoothing_val)\n",
    "    val_ppl   = smooth(val_ppl,  smoothing_val)\n",
    "    val_steps = steps_to_tokens(val_steps)\n",
    "\n",
    "    # Store in final arrays\n",
    "    train_steps_all.append(train_steps)\n",
    "    train_loss_all.append(train_loss)\n",
    "    train_ppl_all.append(train_ppl)\n",
    "\n",
    "    val_steps_all.append(val_steps)\n",
    "    val_loss_all.append(val_loss)\n",
    "    val_ppl_all.append(val_ppl)\n",
    "\n",
    "print(\"\\nAll runs aggregated.\")\n",
    "\n",
    "##############################################################################\n",
    "# 2) FIND THE MINIMUM FINAL STEP ACROSS ALL RUNS & CLIP\n",
    "##############################################################################\n",
    "\n",
    "# We'll look at the last training step for each run,\n",
    "# take the minimum of those \"final steps\", call it `global_end_step`,\n",
    "# and clip each run so we only keep data points up to that step.\n",
    "\n",
    "# 2A) For each run, find final step\n",
    "final_train_steps = []\n",
    "for steps in train_steps_all:\n",
    "    if len(steps) == 0:\n",
    "        final_train_steps.append(0.0)\n",
    "    else:\n",
    "        final_train_steps.append(steps[-1])  # last step in tokens\n",
    "\n",
    "# 2B) global_end_step = minimum of these final steps\n",
    "global_end_step = min(final_train_steps)\n",
    "print(f\"Global end step = {global_end_step:.2f} (tokens)\")\n",
    "\n",
    "# 2C) Clip all runs to that end step\n",
    "def clip_run_to_step(x_vals, y_vals, end_step):\n",
    "    \"\"\"\n",
    "    Keep only the points where x_vals[i] <= end_step.\n",
    "    x_vals and y_vals are same length, sorted in ascending order of x_vals.\n",
    "    \"\"\"\n",
    "    clipped_x, clipped_y = [], []\n",
    "    for (xx, yy) in zip(x_vals, y_vals):\n",
    "        if xx <= end_step:\n",
    "            clipped_x.append(xx)\n",
    "            clipped_y.append(yy)\n",
    "        else:\n",
    "            break\n",
    "    return clipped_x, clipped_y\n",
    "\n",
    "num_runs = len(train_steps_all)\n",
    "for i in range(num_runs):\n",
    "    # Train\n",
    "    old_x = train_steps_all[i]\n",
    "    old_loss = train_loss_all[i]\n",
    "    new_x, new_loss = clip_run_to_step(old_x, old_loss, global_end_step)\n",
    "    train_steps_all[i] = new_x\n",
    "    train_loss_all[i]  = new_loss\n",
    "\n",
    "    old_ppl = train_ppl_all[i]\n",
    "    _, new_ppl = clip_run_to_step(old_x, old_ppl, global_end_step)\n",
    "    train_ppl_all[i] = new_ppl\n",
    "\n",
    "    # Validation\n",
    "    old_valx = val_steps_all[i]\n",
    "    old_vloss= val_loss_all[i]\n",
    "    new_valx, new_vloss = clip_run_to_step(old_valx, old_vloss, global_end_step)\n",
    "    val_steps_all[i] = new_valx\n",
    "    val_loss_all[i]  = new_vloss\n",
    "\n",
    "    old_vppl = val_ppl_all[i]\n",
    "    _, new_vppl = clip_run_to_step(old_valx, old_vppl, global_end_step)\n",
    "    val_ppl_all[i] = new_vppl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/1c1-l-16-r8_1c1_train_loss.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/1c1-l-16-r8_1c1_val_loss.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/1c1-l-16-r8_1c1_train_ppl.pdf\n",
      "Saved plot to: /home/zsarwar/Projects/gpt_neox/gpt-neox/code/plots/1c1-l-16-r8_1c1_val_ppl.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- Plotting ---\n",
    "fig, ax1 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Train Loss\n",
    "for i in range(len(ckpt_paths)):    \n",
    "    ax1.plot(train_steps_all[i], train_loss_all[i], label=legend_keys[i])\n",
    "ax1.set_title(\"Train Loss\")\n",
    "ax1.set_xlabel(\"Tokens (B)\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Save Train Loss plot\n",
    "train_loss_save_path = save_path.replace(\".pdf\", \"_train_loss.pdf\")\n",
    "plt.savefig(train_loss_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {train_loss_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax2 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Validation Loss\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax2.plot(val_steps_all[i], val_loss_all[i], label=legend_keys[i])\n",
    "ax2.set_title(\"Validation Loss\")\n",
    "ax2.set_xlabel(\"Tokens (B)\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "# Save Validation Loss plot\n",
    "val_loss_save_path = save_path.replace(\".pdf\", \"_val_loss.pdf\")\n",
    "plt.savefig(val_loss_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {val_loss_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax3 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Train PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax3.plot(train_steps_all[i], train_ppl_all[i], label=legend_keys[i])\n",
    "ax3.set_title(\"Train Perplexity\")\n",
    "ax3.set_xlabel(\"Tokens (B)\")\n",
    "ax3.set_ylabel(\"PPL\")\n",
    "ax3.grid(True)\n",
    "ax3.legend()\n",
    "\n",
    "# Save Train PPL plot\n",
    "train_ppl_save_path = save_path.replace(\".pdf\", \"_train_ppl.pdf\")\n",
    "plt.savefig(train_ppl_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {train_ppl_save_path}\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax4 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Validation PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax4.plot(val_steps_all[i], val_ppl_all[i], label=legend_keys[i])\n",
    "ax4.set_title(\"Validation Perplexity\")\n",
    "ax4.set_xlabel(\"Tokens (B)\")\n",
    "ax4.set_ylabel(\"PPL\")\n",
    "ax4.grid(True)\n",
    "ax4.legend()\n",
    "\n",
    "# Save Validation PPL plot\n",
    "val_ppl_save_path = save_path.replace(\".pdf\", \"_val_ppl.pdf\")\n",
    "plt.savefig(val_ppl_save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved plot to: {val_ppl_save_path}\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1c1 LORA ENDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "\n",
    "\n",
    "# ckpt_paths = [\n",
    "#     \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12/b001-9aae413635a59a1bd748912a1d6a8077/tensorboard\",\n",
    "#     # \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12/j002-f8d4350954da3c455cfb58e12936d732/tensorboard\",\n",
    "#     \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12/w002-300bc04cda9f137ea77a2da53d51d3f4/tensorboard\",\n",
    "#     # \"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12/k001-e219b0857385c22aca203fa5ca4c6110_2/tensorboard\"\n",
    "# ]\n",
    "# legend_keys = [\n",
    "#     \"1c1 + l-16-r-8\",\n",
    "#     # \"1c1 + l-8_r-4\",\n",
    "#     \"1c1\",\n",
    "#     # \"1c1 + l-16-r-4\",\n",
    "\n",
    "# ]\n",
    "\n",
    "\n",
    "# # === Config ===\n",
    "# ckpt_paths = [\n",
    "#     \"/net/scratch/zsarwar/exps/GPT_experts-8-topk-1-layers12-heads-12/k001-38abff837ec0d231d3e9eb56c9cdb6ac/tensorboard/\",\n",
    "#     # \"/net/scratch/zsarwar/exps/GPT_experts-8-topk-2-layers12-heads-12/k001-bf438c8582fa76d72c2527fa61802b74/tensorboard\",\n",
    "#     \"/net/scratch/zsarwar/exps/GPT_experts-7-topk-1-layers12-heads-12/e002-af3b953a6664974775edb480246a64fc/tensorboard\",\n",
    "#     # \"/net/scratch/zsarwar/exps/GPT_experts-7-topk-1-layers12-heads-12/b003-82d0e08a0c26696f7991aaa0671ae599/tensorboard\"\n",
    "# ]\n",
    "# legend_keys = [\n",
    "#     \"8c1\",\n",
    "#     # \"8c2\",\n",
    "#     \"7c1+Lora8c1\",\n",
    "#     # \"7c1\",\n",
    "\n",
    "# ]\n",
    "\n",
    "\n",
    "# # === Config ===\n",
    "# ckpt_paths = [\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12/w001-446d5e9e78a690921a149c65320e1eea/tensorboard/\",\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12/w001-ec38fe523db7eb59419fcd573e308e8d/tensorboard/\"\n",
    "# ]\n",
    "# legend_keys = [\n",
    "#     \"4c1\",\n",
    "#     \"4c1 + l-16-r-8\"\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# # === Config ===\n",
    "# ckpt_paths = [\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-hetro/e3d7580a3c9ad108011ed2eb4321473f/tensorboard\",\n",
    "# \"/net/scratch/zsarwar/exps/GPT_experts-4-topk-1-layers12-heads-12-base-moe/14a61de6f166d8f3ccae64a958e06558/tensorboard\",\n",
    "# \"/net/scratch/zsarwar/exps_t/GPT_experts-4-topk-1-layers12-heads-12/w001-446d5e9e78a690921a149c65320e1eea/tensorboard\"\n",
    "# ]\n",
    "# legend_keys = [\n",
    "#     \"4c1-hetro\",\n",
    "#     \"4c1 + param-matched\",\n",
    "#     \"4c1\"\n",
    "\n",
    "# ]\n",
    "\n",
    "ckpt_paths = [\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-lora/ad16eaf0ab491faf8f7dff350be0c984/tensorboard\",\n",
    "\"/net/scratch/zsarwar/exps/GPT_experts-1-topk-1-layers12-heads-12-base-moe/0e276af611df1ee7b123423ddae1e8e3/tensorboard\"\n",
    "]\n",
    "legend_keys = [\n",
    "    \"1c1-l-16-r8\",\n",
    "    \"1c1\",\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "key_x = '_'.join(legend_keys)\n",
    "save_path = f\"/home/zsarwar/Projects/neox/gpt-neox-base-moe/code/plots/{key_x}.pdf\"  # Set to None to disable saving\n",
    "\n",
    "\n",
    "def steps_to_tokens(steps):\n",
    "    dp_size = 16\n",
    "    batch_size = 36\n",
    "    seq_len = 2048\n",
    "    divv = 1e9\n",
    "    mult = (batch_size * seq_len * dp_size) / divv\n",
    "    tokens = [step * mult for step in steps]\n",
    "    return tokens\n",
    "\n",
    "ppl_start_threshold = 70\n",
    "smoothing_train = 0.9\n",
    "smoothing_val = 0.4\n",
    "\n",
    "def smooth(vals, factor):\n",
    "    if factor == 0:\n",
    "        return vals\n",
    "    smoothed = []\n",
    "    last = vals[0]\n",
    "    for v in vals:\n",
    "        last = last * factor + (1 - factor) * v\n",
    "        smoothed.append(last)\n",
    "    return smoothed\n",
    "\n",
    "# --- Storage ---\n",
    "train_loss_all = []\n",
    "val_loss_all = []\n",
    "train_ppl_all = []\n",
    "val_ppl_all = []\n",
    "train_steps_all = []\n",
    "val_steps_all = []\n",
    "\n",
    "# --- Load runs ---\n",
    "for path in ckpt_paths:\n",
    "    ea = event_accumulator.EventAccumulator(path)\n",
    "    ea.Reload()\n",
    "\n",
    "    train_loss_events = ea.Scalars(\"train/lm_loss\")\n",
    "    val_loss_events = ea.Scalars(\"validation/lm_loss\")\n",
    "    val_ppl_events = ea.Scalars(\"validation/lm_loss_ppl\")\n",
    "\n",
    "    steps = [e.step for e in train_loss_events]\n",
    "    # if \"e3d7580a3c9ad108011ed2eb4321473f\" in path:\n",
    "        # train_loss = [(e.value - 0.05) for e in train_loss_events]\n",
    "    # else:\n",
    "    train_loss = [e.value for e in train_loss_events]\n",
    "    train_ppl = [np.exp(v) for v in train_loss]\n",
    "\n",
    "    start = next((i for i, v in enumerate(train_ppl) if v < ppl_start_threshold), 0)\n",
    "\n",
    "    steps = steps_to_tokens(steps[start:])\n",
    "\n",
    "    train_loss = smooth(train_loss[start:], smoothing_train)\n",
    "    train_ppl = smooth(train_ppl[start:], smoothing_train)\n",
    "\n",
    "    val_steps = steps_to_tokens([e.step for e in val_loss_events])\n",
    "    val_loss = smooth([e.value for e in val_loss_events], smoothing_val)\n",
    "    val_ppl = smooth([e.value for e in val_ppl_events], smoothing_val)\n",
    "\n",
    "\n",
    "\n",
    "    train_steps_all.append(steps)\n",
    "    val_steps_all.append(val_steps)\n",
    "    train_loss_all.append(train_loss)\n",
    "    val_loss_all.append(val_loss)\n",
    "    train_ppl_all.append(train_ppl)\n",
    "    val_ppl_all.append(val_ppl)\n",
    "\n",
    "\n",
    "\n",
    "# --- Clip to shortest run for training plots ---\n",
    "min_train_len = min(len(steps) for steps in train_steps_all)\n",
    "\n",
    "for i in range(len(train_steps_all)):\n",
    "    train_steps_all[i] = train_steps_all[i][:min_train_len]\n",
    "    train_loss_all[i] = train_loss_all[i][:min_train_len]\n",
    "    train_ppl_all[i] = train_ppl_all[i][:min_train_len]\n",
    "\n",
    "\n",
    "# --- Clip to shortest run for valing plots ---\n",
    "min_val_len = min(len(steps) for steps in val_steps_all)\n",
    "\n",
    "for i in range(len(val_steps_all)):\n",
    "    val_steps_all[i] = val_steps_all[i][:min_val_len]\n",
    "    val_loss_all[i] = val_loss_all[i][:min_val_len]\n",
    "    val_ppl_all[i] = val_ppl_all[i][:min_val_len]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "(ax1, ax2), (ax3, ax4) = axes\n",
    "\n",
    "# Train Loss\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax1.plot(train_steps_all[i], train_loss_all[i], label=legend_keys[i])\n",
    "ax1.set_title(\"Train Loss\")\n",
    "ax1.set_xlabel(\"Tokens (B)\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Validation Loss\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax2.plot(val_steps_all[i], val_loss_all[i], label=legend_keys[i])\n",
    "ax2.set_title(\"Validation Loss\")\n",
    "ax2.set_xlabel(\"Tokens (B)\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "# Train PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax3.plot(train_steps_all[i], train_ppl_all[i], label=legend_keys[i])\n",
    "ax3.set_title(\"Train Perplexity\")\n",
    "ax3.set_xlabel(\"Tokens (B)\")\n",
    "ax3.set_ylabel(\"PPL\")\n",
    "ax3.grid(True)\n",
    "ax3.legend()\n",
    "\n",
    "# Validation PPL\n",
    "for i in range(len(ckpt_paths)):\n",
    "    ax4.plot(val_steps_all[i], val_ppl_all[i], label=legend_keys[i])\n",
    "ax4.set_title(\"Validation Perplexity\")\n",
    "ax4.set_xlabel(\"Tokens (B)\")\n",
    "ax4.set_ylabel(\"PPL\")\n",
    "ax4.grid(True)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# --- Save as PDF if specified ---\n",
    "if save_path:\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot to: {save_path}\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
