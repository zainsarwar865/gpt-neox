WARNING:root:Outstanding DeepSpeed issue means that pp>0, zero1, and bf16 will break without fp32 grads
WARNING:root:Outstanding DeepSpeed issue means that pp>0, zero1, and bf16 will break without fp32 grads
WARNING:root:Outstanding DeepSpeed issue means that pp>0, zero1, and bf16 will break without fp32 grads
WARNING:root:Outstanding DeepSpeed issue means that pp>0, zero1, and bf16 will break without fp32 grads
Using /u/zsarwar/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /u/zsarwar/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /u/zsarwar/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Loading extension module fused_adam...
/u/zsarwar/gpt-neox/gpt-neox/megatron/data/gpt2_dataset.py:222: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
Loading extension module fused_adam...
/u/zsarwar/gpt-neox/gpt-neox/megatron/data/gpt2_dataset.py:222: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
Using /u/zsarwar/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Loading extension module fused_adam...
/u/zsarwar/gpt-neox/gpt-neox/megatron/data/gpt2_dataset.py:222: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
Using /u/zsarwar/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Loading extension module fused_adam...
/u/zsarwar/gpt-neox/gpt-neox/megatron/data/gpt2_dataset.py:222: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
[rank1]: Traceback (most recent call last):
[rank1]:   File "/u/zsarwar/gpt-neox/gpt-neox/c1_configs/bash_scripts/../../train.py", line 34, in <module>
[rank1]:     main()
[rank1]:   File "/u/zsarwar/gpt-neox/gpt-neox/c1_configs/bash_scripts/../../train.py", line 30, in main
[rank1]:     pretrain(neox_args=neox_args)
[rank1]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 229, in pretrain
[rank1]:     iteration = train(
[rank1]:                 ^^^^^^
[rank1]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 925, in train
[rank1]:     loss_dict, skipped_iter = train_step(
[rank1]:                               ^^^^^^^^^^^
[rank1]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 791, in train_step
[rank1]:     timers("forward").stop()
[rank1]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/utils.py", line 249, in stop
[rank1]:     torch.cuda.synchronize()
[rank1]:   File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/torch/cuda/__init__.py", line 954, in synchronize
[rank1]:     return torch._C._cuda_synchronize()
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: KeyboardInterrupt
[rank2]: Traceback (most recent call last):
[rank2]:   File "/u/zsarwar/gpt-neox/gpt-neox/c1_configs/bash_scripts/../../train.py", line 34, in <module>
[rank2]:     main()
[rank2]:   File "/u/zsarwar/gpt-neox/gpt-neox/c1_configs/bash_scripts/../../train.py", line 30, in main
[rank2]:     pretrain(neox_args=neox_args)
[rank2]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 229, in pretrain
[rank2]:     iteration = train(
[rank2]:                 ^^^^^^
[rank2]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 925, in train
[rank2]:     loss_dict, skipped_iter = train_step(
[rank2]:                               ^^^^^^^^^^^
[rank2]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 791, in train_step
[rank2]:     timers("forward").stop()
[rank2]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/utils.py", line 249, in stop
[rank2]:     torch.cuda.synchronize()
[rank2]:   File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/torch/cuda/__init__.py", line 954, in synchronize
[rank2]:     return torch._C._cuda_synchronize()
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: KeyboardInterrupt
[rank3]: Traceback (most recent call last):
[rank3]:   File "/u/zsarwar/gpt-neox/gpt-neox/c1_configs/bash_scripts/../../train.py", line 34, in <module>
[rank3]:     main()
[rank3]:   File "/u/zsarwar/gpt-neox/gpt-neox/c1_configs/bash_scripts/../../train.py", line 30, in main
[rank3]:     pretrain(neox_args=neox_args)
[rank3]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 229, in pretrain
[rank3]:     iteration = train(
[rank3]:                 ^^^^^^
[rank3]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 925, in train
[rank3]:     loss_dict, skipped_iter = train_step(
[rank3]:                               ^^^^^^^^^^^
[rank3]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/training.py", line 791, in train_step
[rank3]:     timers("forward").stop()
[rank3]:   File "/u/zsarwar/gpt-neox/gpt-neox/megatron/utils.py", line 249, in stop
[rank3]:     torch.cuda.synchronize()
[rank3]:   File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/torch/cuda/__init__.py", line 954, in synchronize
[rank3]:     return torch._C._cuda_synchronize()
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: KeyboardInterrupt
Traceback (most recent call last):
  File "/u/zsarwar/gpt-neox/gpt-neox/c1_configs/bash_scripts/../../deepy.py", line 41, in <module>
    main()
  File "/u/zsarwar/gpt-neox/gpt-neox/c1_configs/bash_scripts/../../deepy.py", line 37, in main
    deepspeed.launcher.runner.main(deepspeed_main_args)
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/launcher/runner.py", line 601, in main
    result.wait()
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/subprocess.py", line 2053, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/subprocess.py", line 2011, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x7fb3a875f380>
Traceback (most recent call last):
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 477, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 454, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 183, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 100, in put
    with open(self.file_path + ".tmp", 'wb') as handle:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt: 
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x7f0b1853df80>
Traceback (most recent call last):
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 477, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 455, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_4d_kernel", __class__._4d_kernel)
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 183, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 99, in put
    with FileLock(self.lock_path):
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/filelock/_api.py", line 297, in __enter__
    self.acquire()
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/filelock/_api.py", line 255, in acquire
    self._acquire()
  File "/u/zsarwar/c_envs/c_neox/lib/python3.12/site-packages/filelock/_unix.py", line 39, in _acquire
    fd = os.open(self.lock_file, open_flags, self._context.mode)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt: 
