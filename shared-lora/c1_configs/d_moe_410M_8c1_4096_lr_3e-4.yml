# GPT-2 pretraining setup
{
  # See README for MoE config docs!

  # Have 4 experts per layer (every 2 layers by default)
  "moe_router_type": "topk",
  "moe_num_experts": 8,
  "moe_top_k": 1,
  "moe_jitter_eps" : 0.01,
  # "moe_expert_parallel_size": 8,
  "moe_expert_interval": 1,
  "intermediate_size": 4096,
  "moe_aux_loss_coeff" : 0.01,
  "moe_lora_aux_loss_coeff" : 0.02,
  #"moe_z_loss_coeff": 0.001,
  "lora_rank" : 16,
  "moe_lora_experts" : 8,
  "mlp_type" : "regular",

  # parallelism settings
  # "enable_expert_tensor_parallelism": true,
  "pipe_parallel_size": 0, # not yet supported for MoE
  "model_parallel_size": 1,

  #moe settings 
  # "moe_min_capacity": 4,
  # "moe_train_capacity_factor": 1.0, # Setting to 1.0    
  # "moe_eval_capacity_factor": 1.0, # Setting to 1.0

  # batch / data settings
  "gradient_accumulation_steps": 1,
  "train_micro_batch_size_per_gpu": 32,
  "data_impl": "mmap",

  # model settings
  "num_layers": 24,
  "hidden_size": 1024,
  "num_attention_heads": 16,
  "seq_length": 2048,
  "max_position_embeddings": 2048,
  "norm": "layernorm",
  "pos_emb": "rotary",
  "no_weight_tying": true,
  "gpt_j_residual": false,
  "output_layer_parallelism": "column",

  "attention_config": [[["flash"], 24]],

   # these should provide some speedup but takes a while to build, set to true if desired
   "scaled_upper_triang_masked_softmax_fusion": true,
   "bias_gelu_fusion": false, # not compatible with the new MoE impl
   "rope_fusion": true,

   # init methods
   "init_method": "small_init",
   "output_layer_init_method": "wang_init",

   # optimizer settings
   "optimizer": {
     "type": "Adam",
     "params": {
     "lr": 3.0e-4,
       "betas": [0.9, 0.95],
       "eps": 1.0e-8,
     }
   },
   "min_lr": 3.0e-5,
   "train_iters": 36000,
   "lr_decay_iters": 36000,
   "constant_lr": 1.65e-4, 
   "constant_iters_perc": 0.10,
   "cooldown_iters_perc": 0.7,
   "timescale": 1.0,
   "lr_decay_style": "infinite_cosine",
   "warmup": 0.01,


   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training
   "zero_optimization": {
    "stage": 1,
    "allgather_partitions": True,
    "allgather_bucket_size": 500000000,
    "overlap_comm": True,
    "reduce_scatter": True,
    "reduce_bucket_size": 500000000,
    "contiguous_gradients": True,
  },

   # activation checkpointing
   "checkpoint_activations": true,
   "checkpoint_num_layers": 1,
   "partition_activations": true,
   "synchronize_each_layer": true,

   # regularization
   "gradient_clipping": 1.0,
   "weight_decay": 0.1,
   "hidden_dropout": 0.0,
   "attention_dropout": 0.0,

   # precision settings
  #  "fp16": {
  #    "enabled": true,
  #    "loss_scale": 0,
  #    "loss_scale_window": 1000,
  #    "hysteresis": 2,
  #    "min_loss_scale": 1
  #  },
    "precision": "bfloat16",
    "fp32_allreduce": true,
    "bf16": {
      "enabled": true
    },
    "data_types": {
      "grad_accum_dtype": "bf16"
    },


   # misc. training settings
"distributed_backend": "nccl",
   "checkpoint_factor": 5000,
   "eval_interval": 1000,
   "eval_iters": 10,

   # logging
   "log_interval": 10,
   "steps_per_print": 10,
   "keep_last_n_checkpoints": 4,
   "wall_clock_breakdown": true,

  "no_ssh_check": true,
  #  networking
  # "hostfile": "//uls607/gpt_neox_install/hostfiles/hosts_node"
  "train_dataset_name" : "fineweb_llama2_all",

 "train-data-paths": [
 '//s_/data/fineweb_v2_tokenized_llama2/CC-MAIN-2013-20/CC_MAIN_2013_20_text_document_text_document',
 
 ],

#"train-data-weights" : [0.009298148717968665,0.009728861708470616,0.00928666297037358,0.00853042428598342,0.010425495013447006,0.00885804895604167,0.009998755080228716,0.008550150545141485,0.007409638056660171,0.009013863207618043,0.00829941529220934,0.008122686113632427,0.00659221209813037,0.00907485444598301,0.00886507318349608,0.007391637715620273,0.008308019455479484,0.008324406957408884,0.006477374741496721,0.00785531912424773,0.007895347715845921,0.006921214398479369,0.0069920433454638865,0.005980414306279535,0.008209866586613173,0.008148323712688213,0.008827104739903825,0.010538912682535907,0.010704838976801587,0.01142743012464641,0.011278265006196313,0.013616168735363893,0.013854989638933161,0.009959372770806225,0.011295610618399529,0.011005908775838047,0.011228687434960404,0.01044348538904304,0.012359978178075336,0.010319978245133758,0.010824865307596285,0.011553930432704836,0.0113808510712951,0.010762971881251632,0.010341554704719435,0.009100646609135031,0.010804198441131386,0.01087327606709035,0.009332868118084196,0.010842823193548404,0.010905602038336576,0.010689450286945592,0.011776612816740634,0.010522881822599412,0.010791748087259257,0.009406236856881765,0.009417433902659119,0.01011747595335034,0.009446185913702368,0.009733953096447272,0.010354793436195865,0.009018268282469624,0.00990886942695897,0.009105021321395205,0.00849017965138825,0.011196319856559068,0.008504364679504002,0.010454532827313534,0.00915365516592235,0.01131394477223785,0.00851069709361803,0.012299085360417305,0.008903117191092833,0.009697876201862975,0.013389571405627812,0.010446066614023393,0.012520431288411047,0.010607299440052691,0.010560124879394003,0.013915142570007371,0.01249268269702342,0.013432734404744889,0.01042688847525271,0.011756410185502678,0.015138741515868663,0.013452378325033354,0.010055475785486663,0.013599527162067158,0.014155172915425618,0.015642390811542464,0.0145692484375052,0.015718041180696904,0.015788592799714778,0.015375821968867914,0.010267037980967269,0.009734938236723924],
"train-data-weights" : [1.0],

"test-data-paths": [
 '//s_/data/fineweb_v2_tokenized_llama2/CC-MAIN-2013-20/CC_MAIN_2013_20_text_document_text_document',
  ],
  #"test-data-weights": [0.009298148717968665,0.009728861708470616,0.00928666297037358,0.00853042428598342,0.010425495013447006,0.00885804895604167,0.009998755080228716,0.008550150545141485,0.007409638056660171,0.009013863207618043,0.00829941529220934,0.008122686113632427,0.00659221209813037,0.00907485444598301,0.00886507318349608,0.007391637715620273,0.008308019455479484,0.008324406957408884,0.006477374741496721,0.00785531912424773,0.007895347715845921,0.006921214398479369,0.0069920433454638865,0.005980414306279535,0.008209866586613173,0.008148323712688213,0.008827104739903825,0.010538912682535907,0.010704838976801587,0.01142743012464641,0.011278265006196313,0.013616168735363893,0.013854989638933161,0.009959372770806225,0.011295610618399529,0.011005908775838047,0.011228687434960404,0.01044348538904304,0.012359978178075336,0.010319978245133758,0.010824865307596285,0.011553930432704836,0.0113808510712951,0.010762971881251632,0.010341554704719435,0.009100646609135031,0.010804198441131386,0.01087327606709035,0.009332868118084196,0.010842823193548404,0.010905602038336576,0.010689450286945592,0.011776612816740634,0.010522881822599412,0.010791748087259257,0.009406236856881765,0.009417433902659119,0.01011747595335034,0.009446185913702368,0.009733953096447272,0.010354793436195865,0.009018268282469624,0.00990886942695897,0.009105021321395205,0.00849017965138825,0.011196319856559068,0.008504364679504002,0.010454532827313534,0.00915365516592235,0.01131394477223785,0.00851069709361803,0.012299085360417305,0.008903117191092833,0.009697876201862975,0.013389571405627812,0.010446066614023393,0.012520431288411047,0.010607299440052691,0.010560124879394003,0.013915142570007371,0.01249268269702342,0.013432734404744889,0.01042688847525271,0.011756410185502678,0.015138741515868663,0.013452378325033354,0.010055475785486663,0.013599527162067158,0.014155172915425618,0.015642390811542464,0.0145692484375052,0.015718041180696904,0.015788592799714778,0.015375821968867914,0.010267037980967269,0.009734938236723924],
  "test-data-weights": [1.0],

  "valid-data-paths": [
 '//s_/data/fineweb_v2_tokenized_llama2/CC-MAIN-2013-20/CC_MAIN_2013_20_text_document_text_document',
  ],

  #"valid-data-weights": [0.009298148717968665,0.009728861708470616,0.00928666297037358,0.00853042428598342,0.010425495013447006,0.00885804895604167,0.009998755080228716,0.008550150545141485,0.007409638056660171,0.009013863207618043,0.00829941529220934,0.008122686113632427,0.00659221209813037,0.00907485444598301,0.00886507318349608,0.007391637715620273,0.008308019455479484,0.008324406957408884,0.006477374741496721,0.00785531912424773,0.007895347715845921,0.006921214398479369,0.0069920433454638865,0.005980414306279535,0.008209866586613173,0.008148323712688213,0.008827104739903825,0.010538912682535907,0.010704838976801587,0.01142743012464641,0.011278265006196313,0.013616168735363893,0.013854989638933161,0.009959372770806225,0.011295610618399529,0.011005908775838047,0.011228687434960404,0.01044348538904304,0.012359978178075336,0.010319978245133758,0.010824865307596285,0.011553930432704836,0.0113808510712951,0.010762971881251632,0.010341554704719435,0.009100646609135031,0.010804198441131386,0.01087327606709035,0.009332868118084196,0.010842823193548404,0.010905602038336576,0.010689450286945592,0.011776612816740634,0.010522881822599412,0.010791748087259257,0.009406236856881765,0.009417433902659119,0.01011747595335034,0.009446185913702368,0.009733953096447272,0.010354793436195865,0.009018268282469624,0.00990886942695897,0.009105021321395205,0.00849017965138825,0.011196319856559068,0.008504364679504002,0.010454532827313534,0.00915365516592235,0.01131394477223785,0.00851069709361803,0.012299085360417305,0.008903117191092833,0.009697876201862975,0.013389571405627812,0.010446066614023393,0.012520431288411047,0.010607299440052691,0.010560124879394003,0.013915142570007371,0.01249268269702342,0.013432734404744889,0.01042688847525271,0.011756410185502678,0.015138741515868663,0.013452378325033354,0.010055475785486663,0.013599527162067158,0.014155172915425618,0.015642390811542464,0.0145692484375052,0.015718041180696904,0.015788592799714778,0.015375821968867914,0.010267037980967269,0.009734938236723924],
  "valid-data-weights": [1.0],
  # or for weighted datasets:
  # "train-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],
  # "test-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],
  # "valid-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],
  # "train-data-weights": [1., 2.],
  # "test-data-weights": [2., 1.],
  # "valid-data-weights": [0.5, 0.4],

  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.
  # WARNING: setting this to True will override any user provided weights
  # "weight_by_num_documents": false,
  # "weighted_sampler_alpha": 0.3,

  "vocab_file": "//s_//meta-llama_Llama-2-7b-hf/tokenizer.json",
  # "merge_file": "data/gpt2-merges.txt",
  "tokenizer_type": "HFTokenizer",
  "save": "checkpoints",
  "load": 'none',
  "checkpoint_validation_with_forward_pass": False,

  
  "base_dir": "//vox781/exps/gpt-neox",
  "tensorboard_dir": "tensorboard",
  "log_dir": "logs",
  "use_wandb": False,
  "wandb_host": "https://api.wandb.ai",
  "wandb_project": "neox",
  # "launcher":'pdsh'
  "num_workers": 2,

  "launcher": "torchrun",
  # "memory_breakdown": True,
  # "global_num_gpus": 32,
  # "deepspeed_mpi": true







}
